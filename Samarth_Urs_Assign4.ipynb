{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRZZidh1loTU",
        "outputId": "711198a2-5240-4f78-d96a-6883b7926a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.1.2\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9 (from pyspark==3.1.2)\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880745 sha256=f66efe71dfaa5658555891fc58323f59c519df7f061fffa1112b5a343338f94c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/70/50/7882e1bcb5693225f7cc86698f10953201b48b3f36317c2d18\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.1.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setAppName(\"WikiDictionary\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Set the file paths on your local machine\n",
        "#wikiPagesFile = \"/WikipediaPagesOneDocPerLine1000LinesSmall.txt\"\n",
        "#wikiCategoryFile = \"/wiki-categorylinks-small (1).csv.bz2\"\n",
        "wikiPagesFile = sys.argv[1]\n",
        "wikiCategoryFile = sys.argv[2]\n",
        "\n",
        "# Read two files into RDDs\n",
        "wikiCategoryLinks = sc.textFile(wikiCategoryFile)\n",
        "wikiCats = wikiCategoryLinks.map(lambda x: x.split(\",\")).map(lambda x: (x[0].replace('\"', ''), x[1].replace('\"', '')))\n",
        "\n",
        "wikiPages = sc.textFile(wikiPagesFile)\n",
        "\n",
        "# Assumption: Each document is stored in one line of the text file\n",
        "# We need this count later ...\n",
        "numberOfDocs = wikiPages.count()\n",
        "\n",
        "print(numberOfDocs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX10mST0lpoE",
        "outputId": "ebbe3d3e-6719-4ada-de88-c4adffe2f0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validLines = wikiPages.filter(lambda x: 'id' in x and 'url=' in x)\n",
        "\n",
        "keyAndText = validLines.map(lambda x: (x[x.index('id=\"') + 4: x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\n",
        "\n",
        "def buildArray(listOfIndices):\n",
        "    returnVal = np.zeros(20000)\n",
        "    for index in listOfIndices:\n",
        "        returnVal[index] = returnVal[index] + 1\n",
        "    mysum = np.sum(returnVal)\n",
        "    returnVal = np.divide(returnVal, mysum)\n",
        "    return returnVal\n",
        "\n",
        "keyAndListOfWords = keyAndText.map(lambda x: (str(x[0]), re.compile('[^a-zA-Z]').sub(' ', x[1]).lower().split()))\n",
        "\n",
        "allWords = keyAndListOfWords.flatMap(lambda x: ((j, 1) for j in x[1]))\n",
        "\n",
        "allCounts = allWords.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "topWords = allCounts.top(20000, key=lambda x: x[1])\n",
        "#task 1.1\n",
        "print(\"Top 3 in top 20,000 words:\", topWords[:3])\n",
        "topWordsK = sc.parallelize(range(20000))\n",
        "\n",
        "dictionary = topWordsK.map(lambda x: (topWords[x][0], x))\n",
        "dictionary_broadcast = sc.broadcast(dictionary.collectAsMap())\n",
        "\n",
        "def build_zero_one_array(listOfWords):\n",
        "    returnVal = np.zeros(20000)\n",
        "    for word in listOfWords:\n",
        "        index = dictionary_broadcast.value.get(word)\n",
        "        if index is not None:\n",
        "            returnVal[index] = 1\n",
        "    return returnVal\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def stringVector(x):\n",
        "    returnVal = str(x[0])\n",
        "    for j in x[1]:\n",
        "        returnVal += ',' + str(j)\n",
        "    return returnVal\n",
        "\n",
        "def cousinSim(x, y):\n",
        "    normA = np.linalg.norm(x)\n",
        "    normB = np.linalg.norm(y)\n",
        "    return np.dot(x, y) / (normA * normB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXtaAUDFl2oL",
        "outputId": "2b52fee1-708f-4b36-af12-ae11387f1f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 in top 20,000 words: [('the', 74530), ('of', 34512), ('and', 28479)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "allWordsWithDocID = keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
        "\n",
        "allDictionaryWords = allWordsWithDocID.join(dictionary)\n",
        "\n",
        "justDocAndPos = allDictionaryWords.map(lambda x: (x[1][0], x[1][1]))\n",
        "\n",
        "allDictionaryWordsInEachDoc = justDocAndPos.groupByKey()\n",
        "\n",
        "allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: (x[0], buildArray(x[1])))\n",
        "#task1.2\n",
        "print(\"First 3 elements in RDD with docID and dictionary positions:\", allDocsAsNumpyArrays.take(3))\n",
        "\n",
        "#task 2\n",
        "zeroOrOne = allDocsAsNumpyArrays.map(lambda x: (x[0], build_zero_one_array(x[1])))\n",
        "print(\"First 3 elements in Term Frequency (TF) array:\", zeroOrOne.take(3))\n",
        "\n",
        "dfArray = zeroOrOne.reduce(lambda x1, x2: (\"\", np.add(x1[1], x2[1])))[1]\n",
        "\n",
        "multiplier = np.full(20000, numberOfDocs)\n",
        "\n",
        "idfArray = np.log(np.divide(multiplier, dfArray + 1))\n",
        "print(\"Inverse Document Frequency (IDF) array:\", idfArray)\n",
        "\n",
        "allDocsAsNumpyArraysTFidf = allDocsAsNumpyArrays.map(lambda x: (x[0], np.multiply(x[1], idfArray)))\n",
        "print(\"First 3 elements in TF-IDF matrix of the corpus:\", allDocsAsNumpyArraysTFidf.take(3))\n",
        "\n",
        "featuresRDD = wikiCats.join(allDocsAsNumpyArraysTFidf).map(lambda x: (x[1][0], x[1][1]))\n",
        "\n",
        "featuresRDD.cache()\n",
        "\n",
        "def getPrediction(textInput, k):\n",
        "    myDoc = sc.parallelize(('', textInput))\n",
        "    wordsInThatDoc = myDoc.flatMap(lambda x: ((j, 1) for j in re.compile('[^a-zA-Z]').sub(' ', x).lower().split()))\n",
        "    allDictionaryWordsInThatDoc = dictionary.join(wordsInThatDoc).map(lambda x: (x[1][1], x[1][0])).groupByKey()\n",
        "    myArray = buildArray(allDictionaryWordsInThatDoc.top(1)[0][1])\n",
        "    myArray = np.multiply(myArray, idfArray)\n",
        "    distances = featuresRDD.map(lambda x: (x[0], np.dot(x[1], myArray)))\n",
        "    topK = distances.top(k, lambda x: x[1])\n",
        "    docIDRepresented = sc.parallelize(topK).map(lambda x: (x[0], 1))\n",
        "    numTimes = docIDRepresented.reduceByKey(lambda a, b: a + b)\n",
        "    return numTimes.top(k, lambda x: x[1])\n",
        "\n",
        "#print(getPrediction('Sport Basketball Volleyball Soccer', 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlgE5ksfmDcD",
        "outputId": "d8c76237-df20-4b1e-99f4-7f0e636ade0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 elements in RDD with docID and dictionary positions: [('431952', array([0.08850932, 0.02950311, 0.04658385, ..., 0.        , 0.        ,\n",
            "       0.        ])), ('431971', array([0.08553655, 0.02488336, 0.05132193, ..., 0.        , 0.        ,\n",
            "       0.        ])), ('431989', array([0.09074244, 0.03574702, 0.03849679, ..., 0.        , 0.        ,\n",
            "       0.        ]))]\n",
            "First 3 elements in Term Frequency (TF) array: [('431952', array([0., 0., 0., ..., 0., 0., 0.])), ('431971', array([0., 0., 0., ..., 0., 0., 0.])), ('431989', array([0., 0., 0., ..., 0., 0., 0.]))]\n",
            "Inverse Document Frequency (IDF) array: [6.90775528 6.90775528 6.90775528 ... 6.90775528 6.90775528 6.90775528]\n",
            "First 3 elements in TF-IDF matrix of the corpus: [('431952', array([0.6114007 , 0.20380023, 0.32178984, ..., 0.        , 0.        ,\n",
            "       0.        ])), ('431971', array([0.59086554, 0.17188816, 0.35451932, ..., 0.        , 0.        ,\n",
            "       0.        ])), ('431989', array([0.62682656, 0.24693167, 0.26592642, ..., 0.        , 0.        ,\n",
            "       0.        ]))]\n",
            "[('Disambiguation_pages_with_short_description', 1), ('Human_name_disambiguation_pages', 1), ('Bullfighters', 1), ('Articles_containing_Japanese-language_text', 1), ('Japanese_martial_arts', 1), ('Ko-ryū_bujutsu', 1), ('All_article_disambiguation_pages', 1), ('All_disambiguation_pages', 1), ('Lists_of_sportspeople_by_sport', 1), ('Japanese_swordsmanship', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getPrediction('What is the capital city of Australia?', 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX26bAHHmlgi",
        "outputId": "d6aca7f0-a292-4ba5-c461-7fd9330fdddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('All_set_index_articles', 2), ('Disambiguation_pages_with_short_description', 1), ('Russian_Soviet_Federative_Socialist_Republic', 1), ('All_article_disambiguation_pages', 1), ('All_disambiguation_pages', 1), ('Disambiguation_pages', 1), ('Articles_with_short_description', 1), ('Royal_Navy_ship_names', 1), ('Set_indices_on_ships', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(getPrediction('How many goals Vancouver score last year?', 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzpCK0SprMLr",
        "outputId": "36b9cebe-9f9d-4f0d-ac5f-226302600295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('19th_century_in_science', 2), ('1840s_in_science', 2), ('Articles_with_short_description', 2), ('1841_in_science', 1), ('1830s_in_science', 1), ('1839_in_science', 1), ('1840_in_science', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lrr2AdfQrQGz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}